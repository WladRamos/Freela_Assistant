from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from tavily import TavilyClient
from dotenv import load_dotenv, find_dotenv
import os
import getpass

# Carregar vari√°veis de ambiente
env_file = find_dotenv()
load_dotenv(env_file)

# Configurar chave Tavily
tavily_key = os.environ.get("TAVILY_API_KEY")
if not tavily_key:
    tavily_key = getpass.getpass("Tavily API key:\n")

# Inicializar Tavily Client
tavily_client = TavilyClient(api_key=tavily_key)

# Inicializar LLM
gpt_key = os.getenv('OPENAI_API_KEY')
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1, api_key=gpt_key)

# Lista de sites permitidos para busca no Tavily
ALLOWED_DOMAINS = [
    "https://www.upwork.com/resources",
    "https://www.freelancer.com/articles",
    "https://www.guru.com/blog/"
]

def generate_search_query(user_question, user_info):
    """Gera uma frase em ingl√™s otimizada para a busca."""
    
    prompt_search_query = ChatPromptTemplate.from_messages([
        ("system", """Voc√™ receber√° uma pergunta de um usu√°rio e dever√° gerar uma frase curta e clara em ingl√™s 
        que ser√° usada para buscar artigos sobre o assunto. A frase gerada n√£o precisa ser uma tradu√ß√£o literal, 
        mas deve capturar a inten√ß√£o principal da pergunta.
        Um conjunto de informa√ß√µes do usu√°rio ser√£o passadas junto com a pergunta, e voc√™ pode us√°-las para gerar a frase que ser√° usada na busca, caso fa√ßa sentido.
        Apenas use as informa√ß√µes do usu√°rio se a pergunta fizer refer√™ncia a elas.

        Exemplos:
        - Pergunta: "Boa noite, gostaria de entender melhor como funcionam os frameworks, para que servem, etc. Pois estou tendo d√∫vidas para iniciar o meu projeto."
          -> Frase gerada: "How do frameworks work?"
        
        - Pergunta: "Quais s√£o as melhores linguagens para desenvolvimento web?"
          -> Frase gerada: "Best programming languages for web development"

        - Pergunta: "Sou freelancer e quero dicas para conseguir mais clientes."
          -> Frase gerada: "How to get more clients as a freelancer?"
        
        Gere uma √∫nica frase objetiva para a seguinte pergunta:"""),
        ("user", """"
            - Mensagem: {question}\n
            - user_info: {user_info}
         """)])

    chain_search_query = prompt_search_query | llm

    query = chain_search_query.invoke({"question": user_question, "user_info": user_info}).content.strip()
    query = query.strip('"').strip("'")
    return query

def search_articles_with_tavily(question_in_english):
    """Usa Tavily para buscar artigos apenas nos sites permitidos."""
    
    response = tavily_client.search(
        query=question_in_english,
        include_answer='advanced',
        search_depth='advanced'
    )

    return response

def format_results(search_response):
    """Formata os resultados encontrados no Tavily para servir como contexto."""
    
    if not search_response:
        return "N√£o encontrei informa√ß√µes relevantes"

    context = "### Resumo da Pesquisa\n"

    # üîπ Se houver uma resposta direta do Tavily, us√°-la
    if "answer" in search_response and search_response["answer"]:
        context += f"**{search_response['answer']}**\n\n"

    # üîπ Listar os artigos encontrados
    context += "### Artigos relacionados:\n"
    for result in search_response.get("results", []):
        title = result.get("title", "T√≠tulo n√£o dispon√≠vel")
        url = result.get("url", "#")
        content = result.get("content", "Sem descri√ß√£o dispon√≠vel.")
        context += f"- **[{title}]({url})**\n  - {content}\n\n"

    return context

system = """Baseado nos seguintes trechos de artigos e informa√ß√µes coletadas, responda √† pergunta do usu√°rio.
Um conjunto de informa√ß√µes do usu√°rio ser√° passado junto com a pergunta, e voc√™ pode us√°-las caso necess√°rio.
Escreva sua resposta de forma clara e objetiva, utilizando markdown para formata√ß√£o.
Caso existam links relevantes no Search context, inclua-os no final da resposta, numa se√ß√£o 'Fontes'."""

def generate_final_answer(user_question, search_context, user_info, context):
    """Usa GPT-4o-mini para gerar a resposta final com base nas informa√ß√µes do Tavily."""
    
    prompt = f"System: {system}\n\n {context} \n\nHuman: {user_question}\n\nUser info: {user_info}\n\nSearch context: {search_context}"
    print(prompt)

    response = llm.invoke(input=prompt)
    return response.content

def stream_answer_user_question(user_question, user_info, context):
    question_in_english = generate_search_query(user_question, user_info)
    search_response = search_articles_with_tavily(question_in_english)
    search_context = format_results(search_response)
    print(search_context)

    prompt = f"System: {system}\n\n {context} \n\nHuman: {user_question}\n\nUser info: {user_info}\n\nSearch context: {search_context}"

    for chunk in llm.stream(prompt):
        yield chunk.content
