from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from tavily import TavilyClient
from dotenv import load_dotenv, find_dotenv
import os
import getpass

# Carregar variÃ¡veis de ambiente
env_file = find_dotenv()
load_dotenv(env_file)

# Configurar chave Tavily
tavily_key = os.environ.get("TAVILY_API_KEY")
if not tavily_key:
    tavily_key = getpass.getpass("Tavily API key:\n")

# Inicializar Tavily Client
tavily_client = TavilyClient(api_key=tavily_key)

# Inicializar LLM
gpt_key = os.getenv('OPENAI_API_KEY')
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1, api_key=gpt_key)

# Lista de sites permitidos para busca no Tavily
ALLOWED_DOMAINS = [
    "https://www.upwork.com/resources",
    "https://www.freelancer.com/articles",
    "https://www.guru.com/blog/"
]

def generate_search_query(user_question, user_info):
    """Gera uma frase em inglÃªs otimizada para a busca."""
    
    prompt_search_query = ChatPromptTemplate.from_messages([
        ("system", """VocÃª receberÃ¡ uma pergunta de um usuÃ¡rio e deverÃ¡ gerar uma frase curta e clara em inglÃªs 
        que serÃ¡ usada para buscar artigos sobre o assunto. A frase gerada nÃ£o precisa ser uma traduÃ§Ã£o literal, 
        mas deve capturar a intenÃ§Ã£o principal da pergunta.
        Um conjunto de informaÃ§Ãµes do usuÃ¡rio serÃ£o passadas junto com a pergunta, e vocÃª pode usÃ¡-las para gerar a frase que serÃ¡ usada na busca, caso faÃ§a sentido.

        Exemplos:
        - Pergunta: "Boa noite, gostaria de entender melhor como funcionam os frameworks, para que servem, etc. Pois estou tendo dÃºvidas para iniciar o meu projeto."
          -> Frase gerada: "How do frameworks work?"
        
        - Pergunta: "Quais sÃ£o as melhores linguagens para desenvolvimento web?"
          -> Frase gerada: "Best programming languages for web development"

        - Pergunta: "Sou freelancer e quero dicas para conseguir mais clientes."
          -> Frase gerada: "How to get more clients as a freelancer?"
        
        Gere uma Ãºnica frase objetiva para a seguinte pergunta:"""),
        ("user", """"
            - Mensagem: {question}\n
            - user_info: {user_info}
         """)])

    chain_search_query = prompt_search_query | llm
    return chain_search_query.invoke({"question": user_question, "user_info": user_info}).content.strip()

def search_articles_with_tavily(question_in_english):
    """Usa Tavily para buscar artigos apenas nos sites permitidos."""
    
    response = tavily_client.search(
        query=question_in_english,
        include_answer=True,
        include_domains=ALLOWED_DOMAINS
    )

    return response

def format_results(search_response):
    """Formata os resultados encontrados no Tavily para servir como contexto."""
    
    if not search_response:
        return "NÃ£o encontrei informaÃ§Ãµes relevantes"

    context = "### Resumo da Pesquisa\n"

    # ðŸ”¹ Se houver uma resposta direta do Tavily, usÃ¡-la
    if "answer" in search_response and search_response["answer"]:
        context += f"**{search_response['answer']}**\n\n"

    # ðŸ”¹ Listar os artigos encontrados
    context += "### Artigos relacionados:\n"
    for result in search_response.get("results", []):
        title = result.get("title", "TÃ­tulo nÃ£o disponÃ­vel")
        url = result.get("url", "#")
        content = result.get("content", "Sem descriÃ§Ã£o disponÃ­vel.")
        context += f"- **[{title}]({url})**\n  - {content}\n\n"

    return context

def generate_final_answer(user_question, search_context, user_info):
    """Usa GPT-4o-mini para gerar a resposta final com base nas informaÃ§Ãµes do Tavily."""
    
    prompt_answer = ChatPromptTemplate.from_messages([
        ("system", f"""Baseado nos seguintes trechos de artigos e informaÃ§Ãµes coletadas, responda Ã  pergunta do usuÃ¡rio.
         Um conjunto de informaÃ§Ãµes do usuÃ¡rio serÃ£o passadas junto com a pergunta, e vocÃª pode usÃ¡-las caso necessÃ¡rio.
        Escreva sua resposta de forma clara e objetiva, utilizando markdown para formataÃ§Ã£o.
        Ao final da sua resposta, coloque as fontes que voce utilizou para responder a pergunta.

        {search_context}"""),
        ("user", """"
            - Mensagem: {question}\n
            - user_info: {user_info}
         """)])

    chain_answer = prompt_answer | llm
    return chain_answer.invoke({"question": user_question, "user_info": user_info}).content.strip()

def answer_user_question(user_question, user_info):
    """Fluxo completo: Pergunta -> Busca Tavily -> Responde usando GPT-4o-mini"""
    
    # 1. Converter pergunta para inglÃªs
    question_in_english = generate_search_query(user_question, user_info)

    print("Question in English: ", question_in_english)
    
    # 2. Fazer busca no Tavily restrita aos domÃ­nios permitidos
    search_response = search_articles_with_tavily(question_in_english)

    print(search_response)
    
    # 3. Formatar os resultados para servir como contexto
    search_context = format_results(search_response)
    
    # 4. Gerar a resposta final usando GPT-4o-mini
    final_answer = generate_final_answer(user_question, search_context, user_info)
    
    return final_answer
